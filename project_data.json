{
  "project_name": "Interactive Session",
  "tokens_used": 14693,
  "web_resources": [
    {
      "url": "https://work360.lk",
      "title": "Work360",
      "summary": "Work360 is a **cloud-based Enterprise Resource Planning (ERP) system** designed as an all-in-one platform to streamline and integrate business processes for small to medium-sized businesses, startups, and growing enterprises.\n\n**Key Points for a Software Development Project:**\n\n1.  **Core Functionality (Modules/Features):**\n    *   **Financial Management:** Accounting, expense/revenue tracking, budgeting.\n    *   **HR & Payroll:** Employee data, attendance, payroll processing.\n    *   **Sales & CRM:** Lead tracking, customer relationship management, dedicated customer service platform.\n    *   **Project Management:** Advanced features like Gantt charts, task dependencies, resource allocation, real-time task assignment, file sharing, team communication, approvals, comments, and version tracking.\n    *   **Inventory Control:** Stock monitoring, automated reordering.\n    *   **E-commerce & POS:** Tools for managing online sales and point-of-sale operations.\n    *   **Marketing Automation:** Personalized email design, campaign performance tracking, \"AI growth assistant\" features for conversion optimization.\n    *   **Reporting & Analytics:** Real-time dashboards and customizable reports.\n\n2.  **Technical Architecture & Deployment:**\n    *   **Cloud-Native Solution:** Operates on secure cloud servers, accessible from anywhere with an internet connection.\n    *   **No On-Premises Hardware:** Eliminates the need for complex installations or physical infrastructure management.\n    *   **Access Methods:** Primarily via web browser; mobile app access is also implied.\n    *   **Real-time Operations:** Designed for real-time access to data, approvals, and team communication.\n\n3.  **Security & Data Management Best Practices:**\n    *   **Industry-Standard Encryption:** Data protection during transit and at rest.\n    *   **Multi-Factor Authentication (MFA):** Enhances user login security.\n    *   **Regular Security Updates:** Commitment to maintaining a secure environment.\n    *   **Automated Backups:** Ensures data safety and recoverability on reliable cloud infrastructure.\n    *   **Data Protection Regulation Compliance (GDPR):** Explicitly mentioned as a key consideration for European users, indicating adherence to strict data privacy standards.\n\n4.  **Important Concepts & Design Principles:**\n    *   **Business Process Integration:** Aims to eliminate the hassle of managing multiple disparate software tools by consolidating functions into one platform.\n    *   **Scalability:** Designed to support growing enterprises and scale operations.\n    *   **Customizability:** Adaptable to various industries (manufacturing, retail, services, etc.).\n    *   **\"Smart Features\":** Includes variables and conditional logic, likely for workflow automation, dynamic content generation, or error reduction in various modules (e.g., email marketing, project workflows).\n    *   **User Experience Focus:** Emphasizes making work easier and more efficient, promoting team collaboration and user satisfaction.\n    *   **API/Integration Capabilities:** Explicitly mentions connecting with \"third-party apps like email, e-commerce, or accounting software,\" indicating an open architecture for external integrations.",
      "timestamp": "2025-07-22T13:31:07.053560"
    },
    {
      "id": "214b6f908bb8315b0e3e4f7dd6807106",
      "url": "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash",
      "domain": "ai.google.dev",
      "title": "Gemini models \u00a0|\u00a0 Gemini API \u00a0|\u00a0 Google AI for Developers",
      "description": "Learn about Google&#39;s most advanced AI models including Gemini 2.5 Pro",
      "content": "Home Gemini API Models Gemini models 2.5 Pro spark Our most powerful thinking model with maximum response accuracy and state-of-the-art performance Input audio, images, video, and text, get text responses Tackle difficult problems, analyze large databases, and more Best for complex coding, reasoning, and multimodal understanding 2.5 Flash spark Our best model in terms of price-performance, offering well-rounded capabilities. Input audio, images, video, and text, and get text responses Model thinks as needed; or, you can configure a thinking budget Best for low latency, high volume tasks that require thinking 2.5 Flash-Lite experiment A Gemini 2.5 Flash model optimized for cost efficiency and low latency. Input audio, images, video, and text, and get text responses Most cost-efficient model supporting high throughput Best for real time, low latency use cases Model variants The Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available: You can view the rate limits for each model on the rate limits page. Gemini 2.5 Pro Gemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. Try in Google AI Studio Inputs Audio, images, video, text, and PDF Output Text Input token limit 1,048,576 Output token limit 65,536 Structured outputs Supported Caching Supported Function calling Supported Code execution Supported Search grounding Supported Image generation Not supported Audio generation Not supported Live API Not supported Thinking Supported Batch Mode Supported Stable: gemini-2.5-pro Gemini 2.5 Flash Our best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases. Try in Google AI Studio Inputs Text, images, video, audio Output Text Input token limit 1,048,576 Output token limit 65,536 Audio generation Not supported Caching Supported Code execution Supported Function calling Supported Image generation Not supported Search grounding Supported Structured outputs Supported Thinking Supported Batch Mode Supported Stable: gemini-2.5-flash Preview: gemini-2.5-flash-preview-05-20 Gemini 2.5 Flash-Lite Preview A Gemini 2.5 Flash model optimized for cost efficiency and low latency. Try in Google AI Studio Inputs Text, images, video, and audio Output Text Input token limit 1,000,000 Output token limit 64,000 Structured outputs Supported Caching Supported Function calling Supported Code execution Supported URL Context Supported Search grounding Supported Image generation Not supported Audio generation Not supported Live API Not supported Thinking Supported Preview: gemini-2.5-flash-lite-preview-06-17 Gemini 2.5 Flash Native Audio Our native audio dialog models, with and without thinking, available through the Live API. These models provide interactive and unstructured conversational experiences, with style and control prompting. Try native audio in Google AI Studio Inputs Audio, video, text Output Audio and text Input token limit 128,000 Output token limit 8,000 Audio generation Supported Caching Not supported Code execution Not supported Function calling Supported Image generation Not supported Search grounding Supported Structured outputs Not supported Thinking Supported Tuning Not supported Preview: gemini-2.5-flash-preview-05-20 Experimental: gemini-2.5-flash-exp-native-audio-thinking-dialog Gemini 2.5 Flash Preview Text-to-Speech Gemini 2.5 Flash Preview TTS is our price-performant text-to-speech model, delivering high control and transparency for structured workflows like podcast generation, audiobooks, customer support, and more. Gemini 2.5 Flash rate limits are more restricted since it is an experimental / preview model. Try in Google AI Studio Inputs Text Output Audio Input token limit 8,000 Output token limit 16,000 Structured outputs Not supported Caching Not supported Tuning Not supported Function calling Not supported Code execution Not supported Search Not supported Audio generation Supported Live API Not supported Thinking Not supported gemini-2.5-flash-preview-tts Gemini 2.5 Pro Preview Text-to-Speech Gemini 2.5 Pro Preview TTS is our most powerful text-to-speech model, delivering high control and transparency for structured workflows like podcast generation, audiobooks, customer support, and more. Gemini 2.5 Pro rate limits are more restricted since it is an experimental / preview model. Try in Google AI Studio Inputs Text Output Audio Input token limit 8,000 Output token limit 16,000 Structured outputs Not supported Caching Not supported Tuning Not supported Function calling Not supported Code execution Not supported Search Not supported Audio generation Supported Live API Not supported Thinking Not supported gemini-2.5-pro-preview-tts Gemini 2.0 Flash Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, and a 1M token context window. Try in Google AI Studio Inputs Audio, images, video, and text Output Text Input token limit 1,048,576 Output token limit 8,192 Structured outputs Supported Caching Supported Tuning Not supported Function calling Supported Code execution Supported Search Supported Image generation Not supported Audio generation Not supported Live API Supported Thinking Experimental Batch Mode Supported Latest: gemini-2.0-flash Stable: gemini-2.0-flash-001 Experimental: gemini-2.0-flash-exp Gemini 2.0 Flash Preview Image Generation Gemini 2.0 Flash Preview Image Generation delivers improved image generation features, including generating and editing images conversationally. Try in Google AI Studio Inputs Audio, images, video, and text Output Text and images Input token limit 32,000 Output token limit 8,192 Structured outputs Supported Caching Supported Tuning Not supported Function calling Not supported Code execution Not Supported Search Not Supported Image generation Supported Audio generation Not supported Live API Not Supported Thinking Not Supported Preview: gemini-2.0-flash-preview-image-generation gemini-2.0-flash-preview-image-generation is not currently supported in a number of countries in Europe, Middle East & Africa Gemini 2.0 Flash-Lite A Gemini 2.0 Flash model optimized for cost efficiency and low latency. Try in Google AI Studio Inputs Audio, images, video, and text Output Text Input token limit 1,048,576 Output token limit 8,192 Structured outputs Supported Caching Supported Tuning Not supported Function calling Supported Code execution Not supported Search Not supported Image generation Not supported Audio generation Not supported Live API Not supported Batch API Supported Latest: gemini-2.0-flash-lite Stable: gemini-2.0-flash-lite-001 Gemini 1.5 Flash Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. Try in Google AI Studio Inputs Audio, images, video, and text Output Text Input token limit 1,048,576 Output token limit 8,192 Maximum number of images per prompt 3,600 Maximum video length 1 hour Maximum audio length Approximately 9.5 hours System instructions Supported JSON mode Supported JSON schema Supported Adjustable safety settings Supported Caching Supported Tuning Supported Function calling Supported Code execution Supported Live API Not supported Latest: gemini-1.5-flash-latest Latest stable: gemini-1.5-flash Stable: gemini-1.5-flash-001 gemini-1.5-flash-002 Gemini 1.5 Flash-8B Gemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. Try in Google AI Studio Inputs Audio, images, video, and text Output Text Input token limit 1,048,576 Output token limit 8,192 Maximum number of images per prompt 3,600 Maximum video length 1 hour Maximum audio length Approximately 9.5 hours System instructions Supported JSON mode Supported JSON schema Supported Adjustable safety settings Supported Caching Supported Tuning Supported Function calling Supported Code execution Supported Live API Not supported Latest: gemini-1.5-flash-8b-latest Latest stable: gemini-1.5-flash-8b Stable: gemini-1.5-flash-8b-001 Gemini 1.5 Pro Try Gemini 2.5 Pro Preview, our most advanced Gemini model to date. Gemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. Try in Google AI Studio Inputs Audio, images, video, and text Output Text Input token limit 2,097,152 Output token limit 8,192 Maximum number of images per prompt 7,200 Maximum video length 2 hours Maximum audio length Approximately 19 hours System instructions Supported JSON mode Supported JSON schema Supported Adjustable safety settings Supported Caching Supported Tuning Not supported Function calling Supported Code execution Supported Live API Not supported Latest: gemini-1.5-pro-latest Latest stable: gemini-1.5-pro Stable: gemini-1.5-pro-001 gemini-1.5-pro-002 Imagen 4 Imagen 4 is our latest image model, capable of generating highly detailed images with rich lighting, significantly better text rendering, and higher resolution output than previous models. Gemini API imagen-4.0-generate-preview-06-06 imagen-4.0-ultra-generate-preview-06-06 Input Text Output Images Input token limit 480 tokens (text) Output images 1 (Ultra)1 to 4 (Standard) Imagen 3 Imagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. Gemini API imagen-3.0-generate-002 Input Text Output Images Input token limit N/A Output images Up to 4 Veo 3 Preview The Veo 3 Preview is our latest text-to-video model, capable of generating detailed videos with integrated audio, enhanced prompt adherence, and direct camera controls. Try Veo 3 Gemini API veo-3.0-generate-preview Input Text Output Video with audio Text input 1,024 tokens Output video 1 Veo 2 Veo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. Gemini API veo-2.0-generate-001 Input Text, image Output Video Text input N/A Image input Any image resolution and aspect ratio up to 20MB file size Output video Up to 2 Gemini 2.5 Flash Live The Gemini 2.5 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. Try in Google AI Studio Inputs Audio, video, and text Output Text, and audio Input token limit 1,048,576 Output token limit 8,192 Structured outputs Supported Tuning Not supported Function calling Supported Code execution Supported Search Supported Image generation Not supported Audio generation Supported Thinking Not supported Preview: gemini-live-2.5-flash-preview Gemini 2.0 Flash Live The Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. Try in Google AI Studio Inputs Audio, video, and text Output Text, and audio Input token limit 1,048,576 Output token limit 8,192 Structured outputs Supported Tuning Not supported Function calling Supported Code execution Supported Search Supported Image generation Not supported Audio generation Supported Thinking Not supported Preview: gemini-2.0-flash-live-001 Gemini Embedding The Gemini Embedding model achieves a SOTA performance across many key dimensions including code, multi-lingual, and retrieval. Gemini API gemini-embedding-001 Input Text Output Text embeddings Input token limit 2,048 Output dimension size Flexible, supports: 128 - 3072, Recommended: 768, 1536, 3072 Stable: gemini-embedding-001 Preview: gemini-embedding-exp-03-07 Legacy Embedding Models Text embeddings are used to measure the relatedness of strings and are widely used in many AI applications. Gemini API models/text-embedding-004 Input Text Output Text embeddings Input token limit 2,048 Output dimension size 768 See the examples to explore the capabilities of these model variations. [*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. Model version name patterns Gemini models are available in either stable, preview, or experimental versions. In your code, you can use one of the following model name formats to specify which model and version you want to use. Latest stable Points to the most recent stable version released for the specified model generation and variation. To specify the latest stable version, use the following pattern: <model>-<generation>-<variation>. For example, gemini-2.0-flash. Stable Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model. To specify a stable version, use the following pattern: <model>-<generation>-<variation>-<version>. For example, gemini-2.0-flash-001. Preview Points to a preview model which may not be suitable for production use, come with more restrictive rate limits, but may have billing enabled. To specify a preview version, use the following pattern: <model>-<generation>-<variation>-<version>. For example, gemini-2.5-pro-preview-06-05. Preview models are not stable and availability of model endpoints is subject to change. Experimental Points to an experimental model which may not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly. To specify an experimental version, use the following pattern: <model>-<generation>-<variation>-<version>. For example, gemini-2.0-pro-exp-02-05. Experimental models are not stable and availability of model endpoints is subject to change. Experimental models In addition to stable models, the Gemini API offers experimental models which may not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future. Previous experimental models As new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version: Supported languages Gemini models are trained to work with the following languages: Arabic (ar) Bengali (bn) Bulgarian (bg) Chinese simplified and traditional (zh) Croatian (hr) Czech (cs) Danish (da) Dutch (nl) English (en) Estonian (et) Finnish (fi) French (fr) German (de) Greek (el) Hebrew (iw) Hindi (hi) Hungarian (hu) Indonesian (id) Italian (it) Japanese (ja) Korean (ko) Latvian (lv) Lithuanian (lt) Norwegian (no) Polish (pl) Portuguese (pt) Romanian (ro) Russian (ru) Serbian (sr) Slovak (sk) Slovenian (sl) Spanish (es) Swahili (sw) Swedish (sv) Thai (th) Turkish (tr) Ukrainian (uk) Vietnamese (vi)",
      "summary": "This content outlines Google's Gemini API models, providing a comprehensive overview of their capabilities, ideal use cases, and technical specifications crucial for software development projects.\n\n**Key Points for Software Development Projects:**\n\n1.  **Model Tiers & Use Cases:**\n    *   **Gemini 2.5 Pro:** Google's most powerful and accurate model. Best for complex coding, advanced reasoning, analyzing large datasets/codebases/documents, and multimodal understanding (audio, images, video, text input; text output).\n    *   **Gemini 2.5 Flash:** Best price-performance model. Ideal for low-latency, high-volume tasks that require thinking, large-scale processing, and agentic use cases. Supports multimodal input (audio, images, video, text input; text output). Offers a configurable \"thinking budget.\"\n    *   **Gemini 2.5 Flash-Lite:** Optimized for cost efficiency and low latency. Most cost-efficient for high-throughput, real-time use cases.\n    *   **Specialized Models:**\n        *   **Native Audio / Live API:** For interactive, unstructured conversational experiences (bidirectional voice/video interactions).\n        *   **Text-to-Speech (TTS):** `gemini-2.5-flash-preview-tts`, `gemini-2.5-pro-preview-tts` for generating audio from text (e.g., podcasts, audiobooks, customer support).\n        *   **Image Generation:** `Imagen 3`, `Imagen 4`, `gemini-2.0-flash-preview-image-generation` for generating and editing images.\n        *   **Video Generation:** `Veo 2`, `Veo 3 Preview` for generating videos from text/images, with integrated audio and camera controls.\n        *   **Gemini Embedding:** `gemini-embedding-001` for generating text embeddings, crucial for similarity search, retrieval, and measuring string relatedness. Supports flexible output dimension sizes.\n\n2.  **Technical Capabilities & Features:**\n    *   **Multimodality:** Most core Gemini models support diverse inputs (audio, images, video, text) and output text. Specialized models handle image/video/audio output.\n    *   **Token Limits:** Critical for managing context and response length.\n        *   **Core Gemini (Pro/Flash):** High input token limits (1,048,576 for 2.5 models, 2,097,152 for 1.5 Pro) and output token limits (65,536 for 2.5 models, 8,192 for 1.5 models).\n        *   **Specialized models:** Vary significantly (e.g., TTS ~8k input, 16k output; Embedding ~2k input).\n        *   *Note:* A token is roughly 4 characters or 0.6-0.8 English words.\n    *   **Supported Features (Varies by model):**\n        *   **Function Calling:** Enables models to interact with external tools and APIs.\n        *   **Code Execution:** Allows models to run and debug code.\n        *   **Structured Outputs:** Supports JSON mode and JSON schema for reliable, parseable responses.\n        *   **Caching:** Improves performance for repetitive queries.\n        *   **Search Grounding:** Integrates external search results to reduce hallucinations.\n        *   **Thinking:** Models can perform complex reasoning steps.\n        *   **Batch Mode:** For processing multiple requests efficiently.\n        *   **Live API:** For real-time, low-latency, bidirectional interactions (e.g., voice/video).\n        *   **System Instructions:** For guiding model behavior.\n        *   **Adjustable Safety Settings:** For content moderation.\n        *   **URL Context:** For processing content from URLs (Flash-Lite).\n\n3.  **Model Versioning & Stability (Best Practice):**\n    *   **Latest Stable (`<model>-<generation>-<variation>`, e.g., `gemini-2.0-flash`):** Points to the most recent stable version. Use with caution in production as it might update.\n    *   **Specific Stable (`<model>-<generation>-<variation>-<version>`, e.g., `gemini-2.0-flash-001`):** **Recommended for production applications.** These models are static and do not change, ensuring consistent behavior.\n    *   **Preview (`-preview-`):** Early access models, not suitable for production. May have restrictive rate limits and are subject to change or removal. Billing enabled.\n    *   **Experimental (`-exp-`):** Highly unstable, not for production. Released for feedback and latest updates. Can be swapped without notice and no guarantee of becoming stable.\n\n4.  **Considerations for Project Planning:**\n    *   **Model Selection:** Choose the model that best balances required intelligence/accuracy, cost, and latency for your specific task.\n    *   **Production Readiness:** Prioritize `specific stable` versions for production deployments. Use `preview` and `experimental` models only for testing, exploration, or non-critical features due to their instability and rate limits.\n    *   **Rate Limits:** Be aware that preview and experimental models have more restrictive rate limits.\n    *   **Input/Output Handling:** Design your application to respect token limits and parse structured outputs where available.\n    *   **Integration:** Leverage features like Function Calling and Code Execution for powerful tool integration.",
      "timestamp": "2025-07-22T14:00:36.193493",
      "content_type": "text/html",
      "status_code": 200,
      "links": [
        "#main-content",
        "/",
        "https://developers.googleblog.com/en/veo-3-now-available-gemini-api/",
        "https://ai.google.dev/",
        "https://ai.google.dev/gemini-api",
        "https://ai.google.dev/gemini-api/docs",
        "#gemini-2.5-pro",
        "#gemini-2.5-flash",
        "#gemini-2.5-flash-lite",
        "/gemini-api/docs/thinking",
        "#gemini-2.5-pro",
        "#gemini-2.5-flash",
        "#gemini-2.5-flash-lite",
        "#gemini-2.5-flash-native-audio",
        "#gemini-2.5-flash-preview-tts",
        "#gemini-2.5-pro-preview-tts",
        "#gemini-2.0-flash",
        "#gemini-2.0-flash-preview-image-generation",
        "#gemini-2.0-flash-lite",
        "#gemini-1.5-flash",
        "#gemini-1.5-flash-8b",
        "#gemini-1.5-pro",
        "#gemini-embedding",
        "#imagen-4",
        "#imagen-3",
        "#veo-3",
        "#veo-2",
        "#live-api",
        "#live-api-2.0",
        "/gemini-api/docs/rate-limits",
        "https://aistudio.google.com?model=gemini-2.5-pro",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "https://aistudio.google.com?model=gemini-2.5-flash",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "https://aistudio.google.com?model=gemini-2.5-flash-lite-preview-06-17",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "/gemini-api/docs/live",
        "https://aistudio.google.com/app/live",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "https://aistudio.google.com/generate-speech",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "https://aistudio.google.com/generate-speech",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "https://aistudio.google.com?model=gemini-2.0-flash-001",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "https://aistudio.google.com?model=gemini-2.0-flash-preview-image-generation",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "https://aistudio.google.com?model=gemini-2.0-flash-lite",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "https://aistudio.google.com?model=gemini-1.5-flash",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "https://aistudio.google.com?model=gemini-1.5-flash",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "/gemini-api/docs/models/experimental-models#available-models",
        "https://aistudio.google.com?model=gemini-1.5-pro",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "#token-size",
        "#token-size",
        "https://deepmind.google/models/veo/",
        "https://aistudio.google.com?model=gemini-live-2.5-flash-preview",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "https://aistudio.google.com?model=gemini-2.0-flash-live-001",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "https://deepmind.google/research/publications/157741/",
        "#token-size",
        "/gemini-api/docs/models/gemini#model-versions",
        "/gemini-api/docs/models#gemini-embedding",
        "/gemini-api/docs/embeddings",
        "#token-size",
        "#rate-limits",
        "/examples",
        "https://creativecommons.org/licenses/by/4.0/",
        "https://www.apache.org/licenses/LICENSE-2.0",
        "https://developers.google.com/site-policies"
      ],
      "word_count": 2188,
      "project": "default"
    }
  ]
}